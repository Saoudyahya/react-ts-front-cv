# ğŸ”¬ Multi-Modal Fusion & Directional Navigation System

## ğŸ“‹ Table of Contents
- [Overview](#overview)
- [Multi-Modal Fusion Architecture](#multi-modal-fusion-architecture)
- [Directional Navigation System](#directional-navigation-system)
- [Download & Integration](#download--integration)

---

## ğŸŒŸ Overview

This vision navigation system combines **cutting-edge AI fusion technology** with **intelligent spatial awareness** to provide real-time guidance for visually impaired users.

### Key Features
- ğŸ§  **Multi-Modal Fusion**: Combines spatial + semantic understanding
- ğŸ§­ **3-Zone Directional System**: LEFT, FRONT, RIGHT awareness
- ğŸ”Š **Voice Guidance**: Speaks navigation instructions aloud
- ğŸ“¦ **Bounding Box Detection**: Visual object tracking
- âš¡ **Real-time Processing**: 3-second auto-refresh capability

---

## ğŸ”¬ Multi-Modal Fusion Architecture

### What is Multi-Modal Fusion?

Multi-modal fusion combines **two different types of visual information** to create a richer, more accurate understanding of the scene:

1. **Spatial Information** (from YOLO) - *WHERE* things are
2. **Semantic Information** (from ViT) - *WHAT* things mean

Think of it like combining a GPS (location) with Google (knowledge) to get both position AND context.

---

### Architecture Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INPUT IMAGE                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                   â”‚
         â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   YOLO Model    â”‚   â”‚   ViT Model     â”‚
â”‚   (YOLOv9c)     â”‚   â”‚   (ViT-Base)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                   â”‚
         â”‚ Spatial           â”‚ Semantic
         â”‚ Features          â”‚ Features
         â”‚ [N, 8]            â”‚ [1, 768]
         â”‚                   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Fusion Module    â”‚
         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
         â”‚  â”‚ Projection   â”‚ â”‚
         â”‚  â”‚ Layers       â”‚ â”‚
         â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
         â”‚  â”‚ Cross-       â”‚ â”‚
         â”‚  â”‚ Attention    â”‚ â”‚
         â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
         â”‚  â”‚ Layer Norm   â”‚ â”‚
         â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
         â”‚  â”‚ Fusion       â”‚ â”‚
         â”‚  â”‚ Layer        â”‚ â”‚
         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
           Fused Features [256]
                   â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Adapter Layer   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   GPT-2 / Mini    â”‚
         â”‚   Text Generator  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
         Rich Scene Description
```

---

### Component Breakdown

#### 1. **YOLO Spatial Features** (8 dimensions per object)

For each detected object, YOLO extracts:

```python
spatial_feature = [
    center_x,      # Normalized X position (0-1)
    center_y,      # Normalized Y position (0-1)  
    width,         # Normalized width (0-1)
    height,        # Normalized height (0-1)
    area,          # Object area (width Ã— height)
    aspect_ratio,  # width / height
    confidence,    # Detection confidence (0-1)
    class_id       # Object class (person, car, etc.)
]
```

**Example**: Person detected
```
[0.5, 0.4, 0.2, 0.6, 0.12, 0.33, 0.95, 0]
â”‚    â”‚    â”‚    â”‚    â”‚     â”‚     â”‚     â””â”€ Class: person
â”‚    â”‚    â”‚    â”‚    â”‚     â”‚     â””â”€â”€â”€â”€â”€â”€â”€ 95% confident
â”‚    â”‚    â”‚    â”‚    â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Width/height = 0.33
â”‚    â”‚    â”‚    â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Takes up 12% of frame
â”‚    â”‚    â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 60% of image height
â”‚    â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 20% of image width
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Y: 40% down from top
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ X: Center of frame
```

#### 2. **ViT Semantic Features** (768 dimensions)

ViT (Vision Transformer) processes the entire image to extract high-level semantic understanding:

```python
# ViT extracts from image patches
semantic_features = [768-dimensional vector]
```

This captures:
- Object relationships
- Scene context
- Visual semantics
- Texture and patterns
- Overall composition

#### 3. **Cross-Attention Fusion**

The fusion module uses multi-head attention to combine both:

```python
class MultiModalFusionModule(nn.Module):
    def __init__(self, spatial_dim=8, semantic_dim=768, 
                 hidden_dim=256, num_heads=4):
        
        # Project to same dimension
        self.spatial_projection = Linear(8 â†’ 256)
        self.semantic_projection = Linear(768 â†’ 256)
        
        # Cross-attention mechanism
        self.cross_attention = MultiheadAttention(
            embed_dim=256,
            num_heads=4  # 4 attention heads
        )
        
        # Fusion layer
        self.fusion_layer = Linear(512 â†’ 256)
```

**How it works**:
1. **Project**: Convert both features to same dimension (256)
2. **Attend**: Use attention to find relationships
3. **Fuse**: Concatenate and merge with MLP
4. **Output**: Single 256D feature vector

**Attention Example**:
```
Spatial: "person at X=0.5, size=large"
         â†“ â†˜
        Attention finds:
         â†“   â†˜
Semantic: "indoor scene, sitting posture"
         
Result: "Large person sitting indoors at center"
```

---

### Why Fusion Improves Results

**Without Fusion** (Basic mode):
- Caption: "a person in a room"
- Objects: person (95%), chair (87%)
- Limited context

**With Fusion** (Fusion mode):
- Caption: "a person in a room"  
- Objects: person (95%), chair (87%)
- **Enhanced Description**: "The image shows a person seated comfortably at the center of a well-lit indoor space. A chair is positioned to their right, suggesting a living area or office environment. The person appears to be in a relaxed posture, indicating a casual setting."

**The difference**: Fusion adds spatial reasoning and contextual relationships that weren't possible before.

---

### Performance Comparison

| Mode | Fusion | Time | Description Quality | Use Case |
|------|--------|------|---------------------|----------|
| Basic | âŒ | 0.5s | Object list only | Speed priority |
| GPT-2 Mini | âŒ | 2-3s | Good descriptions | Balanced |
| **GPT-2 Mini + Fusion** | âœ… | 4-6s | Rich, spatial-aware | **Recommended** â­ |
| GPT-2 + Fusion | âœ… | 5-8s | Highest quality | Best quality |

---

## ğŸ§­ Directional Navigation System

### Zone-Based Spatial Awareness

The system divides the camera view into **3 zones** based on object positions:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                      â”‚
â”‚    LEFT      FRONT       RIGHT      â”‚
â”‚   (0-33%)   (33-66%)   (66-100%)    â”‚
â”‚                                      â”‚
â”‚    ğŸŸ§        ğŸŸ¦         ğŸŸ©          â”‚
â”‚                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â¬…ï¸          â¬†ï¸          â¡ï¸
```

**Important**: Camera view is MIRRORED (left/right swap)
- Object on screen's LEFT = user's RIGHT
- Object on screen's RIGHT = user's LEFT

---

### Zone Detection Algorithm

```python
def get_object_zone(object, image_width):
    # Get object center
    center_x = (object.x1 + object.x2) / 2
    
    # Classify zone (MIRRORED)
    if center_x < width / 3:
        return "right"  # Screen left = user's right
    elif center_x > 2 * width / 3:
        return "left"   # Screen right = user's left
    else:
        return "front"  # Center
```

**Example Scene**:
```
Image (640px wide):
- Chair at X=100  â†’ center_x=150  â†’ 150 < 213  â†’ RIGHT zone
- Person at X=320 â†’ center_x=320  â†’ 213-426    â†’ FRONT zone
- Table at X=500  â†’ center_x=550  â†’ 550 > 426  â†’ LEFT zone
```

---

### Navigation Decision Logic

The system generates instructions based on obstacle priority:

```python
def generate_navigation_instructions(objects):
    # Dangerous objects
    dangerous = ['person', 'car', 'truck', 'bicycle', 
                 'motorcycle', 'chair', 'table', 'bench',
                 'couch', 'bed']
    
    # Check each zone
    left_dangers = filter_dangerous(left_zone)
    front_dangers = filter_dangerous(front_zone)
    right_dangers = filter_dangerous(right_zone)
    
    # Decision tree
    if front_dangers:
        if not left_dangers:
            return "Move left" (CAUTION)
        elif not right_dangers:
            return "Move right" (CAUTION)
        else:
            return "STOP!" (DANGER)
    
    elif left_dangers and not right_dangers:
        return "Stay right" (CAUTION)
    
    elif right_dangers and not left_dangers:
        return "Stay left" (CAUTION)
    
    else:
        return "Clear path ahead" (SAFE)
```

---

### Priority Levels

#### ğŸŸ¢ SAFE
- **Condition**: No obstacles detected
- **Message**: "Clear path ahead. Safe to proceed."
- **Visual**: Green border
- **Voice**: "Clear path ahead"

#### ğŸŸ¡ CAUTION  
- **Condition**: Obstacles in non-critical positions
- **Message**: "Obstacle on [left/right]. Stay [right/left]."
- **Visual**: Yellow/orange border
- **Voice**: Speaks the specific direction

#### ğŸ”´ DANGER
- **Condition**: Obstacles blocking all paths
- **Message**: "Stop! Obstacles in all directions."
- **Visual**: Red border with pulsing shadow
- **Voice**: "Stop! Obstacles detected."

---

### Visual Overlay System

The React frontend renders real-time directional guidance:

```typescript
// Zone colors
const ZONE_COLORS = {
  left: '#FF9800',   // Orange
  front: '#2196F3',  // Blue
  right: '#4CAF50'   // Green
}

// Draw on canvas
drawDirectionOverlay() {
  // Highlight zones with objects
  if (leftObjects.length > 0) {
    ctx.fillStyle = 'rgba(255, 152, 0, 0.2)'
    ctx.fillRect(2*width/3, 0, width/3, height)
  }
  
  // Draw bounding boxes
  objects.forEach(obj => {
    // Color by zone
    const color = getZoneColor(obj.center_x)
    ctx.strokeStyle = color
    ctx.strokeRect(x1, y1, width, height)
    
    // Label
    ctx.fillText(`${label} ${confidence}%`, x1, y1)
  })
}
```

---

### Real-World Navigation Examples

#### Example 1: Clear Path
```
Scene: Empty hallway
Objects: []

Navigation Output:
â”œâ”€ Direction: forward
â”œâ”€ Priority: safe
â”œâ”€ Message: "Clear path ahead. Safe to proceed."
â””â”€ Voice: "Clear path ahead"

Visual:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        â”‚
â”‚                        â”‚  â† No colored zones
â”‚                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Example 2: Person Ahead
```
Scene: Person directly in front
Objects: [person at X=320/640]

Navigation Output:
â”œâ”€ Direction: left
â”œâ”€ Priority: caution
â”œâ”€ Message: "Path blocked ahead. Move left."
â”œâ”€ Reason: "Obstacle detected: person"
â””â”€ Voice: "Path blocked ahead. Move left."

Visual:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          ğŸŸ¦            â”‚  â† Front zone (blue)
â”‚       [person]         â”‚     highlighted
â”‚          ğŸŸ¦            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Example 3: Chair on Right Side
```
Scene: Chair to user's right
Objects: [chair at X=100/640]  # Screen left

Navigation Output:
â”œâ”€ Direction: left
â”œâ”€ Priority: caution
â”œâ”€ Message: "Obstacle on right. Stay left."
â”œâ”€ Reason: "Right side: chair"
â””â”€ Voice: "Obstacle on right. Stay left."

Visual:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸŸ©                     â”‚  â† Right zone (green)
â”‚[chair]                 â”‚     with object
â”‚ ğŸŸ©                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Example 4: Multiple Obstacles
```
Scene: Person ahead, table left, chair right
Objects: [person at 320, table at 500, chair at 100]

Navigation Output:
â”œâ”€ Direction: stop
â”œâ”€ Priority: danger
â”œâ”€ Message: "Stop! Obstacles in all directions."
â”œâ”€ Reason: "Multiple obstacles detected"
â””â”€ Voice: "Stop! Obstacles in all directions."

Visual:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸŸ©    ğŸŸ¦      ğŸŸ§       â”‚  â† All zones
â”‚[chair][person][table]  â”‚     highlighted
â”‚ ğŸŸ©    ğŸŸ¦      ğŸŸ§       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Voice Guidance System

The system uses Web Speech API for audio feedback:

```typescript
const speak = (text: string) => {
  if (voiceEnabled) {
    const utterance = new SpeechSynthesisUtterance()
    utterance.text = text
    utterance.rate = 0.9    // Slightly slower
    utterance.pitch = 1.0   // Normal pitch
    utterance.volume = 1.0  // Full volume
    
    speechSynthesis.speak(utterance)
  }
}
```

**Triggered Events**:
- Camera start/stop
- Navigation instruction changes
- Mode enabled/disabled

---

## ğŸ“¥ Download & Integration

### Annotated Images

Each processed frame can be downloaded with bounding boxes:

```typescript
// Download button
<button onClick={() => {
  const link = document.createElement('a')
  link.href = annotatedImage  // Base64 image
  link.download = `annotated_${Date.now()}.jpg`
  link.click()
}}>
  ğŸ’¾ Download Image
</button>
```

**Image Format**: JPEG with:
- Colored bounding boxes (by zone)
- Object labels + confidence scores
- Zone indicators (LEFT/FRONT/RIGHT)

---

### System Integration

#### 1. **Backend API**

```python
# Start camera
POST /camera/start?camera_index=1

# Process frame (with fusion)
POST /process_camera/gpt2-mini-fusion?annotate=true

# Response
{
  "status": "success",
  "data": {
    "caption": "...",
    "detected_objects": [...],
    "guidance": "...",
    "llm_description": "...",
    "fusion_enabled": true,
    "attention_stats": {...}
  },
  "annotated_image_base64": "..."  # Download this
}
```

#### 2. **Frontend Integration**

```typescript
// Auto-processing mode
const [autoProcess, setAutoProcess] = useState(false)

useEffect(() => {
  if (autoProcess && isCameraRunning) {
    const interval = setInterval(() => {
      processFrame()  // Every 3 seconds
    }, 3000)
    return () => clearInterval(interval)
  }
}, [autoProcess, isCameraRunning])
```

#### 3. **Custom Navigation Logic**

```typescript
// Add custom obstacle types
const dangerousObjects = [
  'person', 'car', 'truck', 'bicycle',
  'motorcycle', 'chair', 'table', 'bench',
  'couch', 'bed',
  // Add your own:
  'door', 'stairs', 'elevator'
]

// Modify priority levels
const getPriority = (obstacles) => {
  if (obstacles.includes('stairs')) return 'critical'
  if (obstacles.length > 3) return 'danger'
  if (obstacles.length > 0) return 'caution'
  return 'safe'
}
```

---

## ğŸ¯ Usage Recommendations

### For Developers

1. **Testing**: Use `find_working_cameras.py` to find camera index
2. **Modes**: Start with `gpt2-mini-fusion` for best balance
3. **Auto-Process**: Enable for continuous monitoring
4. **Voice**: Essential for blind users

### For Users

1. **Camera Placement**: Mount at chest/shoulder height
2. **Lighting**: Ensure adequate indoor lighting
3. **Speed**: Walk slowly when using auto-process mode
4. **Voice Volume**: Adjust system volume for clear audio

### Performance Tips

- **CPU Mode**: 4-6 seconds per frame with fusion
- **GPU Mode**: 1-2 seconds per frame (if CUDA available)
- **Auto-Process**: 3-second interval recommended
- **Memory**: 5GB RAM for fusion mode

---

## ğŸ”§ Technical Specifications

### Models Used
- **YOLO**: YOLOv9c (50MB) - Object detection
- **ViT**: ViT-Base-Patch16-224 (350MB) - Image features
- **GPT-2**: Standard (548MB) or DistilGPT-2 (319MB) - Text generation
- **Whisper**: Tiny (75MB) - Audio transcription

### System Requirements
- Python 3.8+
- 8GB RAM minimum (16GB for fusion)
- CPU: 4+ cores recommended
- GPU: Optional but recommended for speed

### Supported Platforms
- âœ… Windows (DirectShow)
- âœ… Linux (V4L2)
- âœ… macOS (AVFoundation)

---

## ğŸ“š Research Background

### Multi-Modal Fusion
Based on modern computer vision research combining:
- **Attention Mechanisms**: Transformer-style cross-attention
- **Feature Pyramid**: Multi-scale spatial features
- **Semantic Segmentation**: Dense visual understanding

### Spatial Reasoning
Inspired by:
- Scene understanding for autonomous vehicles
- Assistive technology for visually impaired
- Human spatial cognition models

---

## ğŸ“ Learn More

### Key Concepts
- **Cross-Attention**: How different feature types interact
- **Feature Fusion**: Combining multiple information sources
- **Spatial Awareness**: Zone-based object localization
- **Priority Scoring**: Risk assessment for navigation

### Papers & Resources
- Vision Transformers (ViT) - Dosovitskiy et al.
- YOLO: Real-Time Object Detection
- Multi-Modal Fusion in Computer Vision
- Assistive Technology for Visual Impairment

---

## ğŸ¤ Contributing

Want to improve the system?

**Fusion Improvements**:
- Add temporal fusion (video sequences)
- Implement depth estimation
- Multi-camera fusion

**Navigation Enhancements**:
- Add gesture recognition
- Implement path planning
- Include elevation detection (stairs, ramps)

---

## ğŸ“„ License

MIT License - Free to use and modify!

---

**Made with â¤ï¸ for accessibility and AI research**

---

## ğŸ‰ Quick Reference Card

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         QUICK REFERENCE CARD                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚  ğŸ”¬ FUSION = Spatial (YOLO) + Semantic (ViT)   â”‚
â”‚                                                 â”‚
â”‚  ğŸ§­ ZONES:  LEFT | FRONT | RIGHT               â”‚
â”‚              ğŸŸ§  |  ğŸŸ¦   |  ğŸŸ©                  â”‚
â”‚                                                 â”‚
â”‚  ğŸš¦ PRIORITY: ğŸŸ¢ Safe â†’ ğŸŸ¡ Caution â†’ ğŸ”´ Danger â”‚
â”‚                                                 â”‚
â”‚  âš¡ MODES:                                      â”‚
â”‚     Basic: 1s (no fusion)                      â”‚
â”‚     GPT-2 Mini + Fusion: 4-6s â­ BEST          â”‚
â”‚                                                 â”‚
â”‚  ğŸ”Š VOICE: Toggle in settings                  â”‚
â”‚                                                 â”‚
â”‚  ğŸ’¾ DOWNLOAD: Click "Download Image" button    â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

**Need Help?** Check the main README or open an issue!
